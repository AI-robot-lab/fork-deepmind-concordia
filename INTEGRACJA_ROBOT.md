# Integracja Concordia z Robotem Unitree G1 EDU-U6

## Wprowadzenie

Ten dokument opisuje jak zintegrowaƒá symulacje Concordia z fizycznym robotem
humanoidalnym Unitree G1 EDU-U6. Robot G1 to zaawansowana platforma humanoidalna
z mo≈ºliwo≈õciami manipulacji, lokomocji i interakcji z otoczeniem.

## Architektura Integracji

### Warstwa 1: Concordia (M√≥zg - Warstwa Decyzyjna)
**Co robi:** Podejmuje inteligentne decyzje na podstawie obserwacji otoczenia.

**Komponenty:**
- Agent reprezentujƒÖcy "umys≈Ç" robota
- Game Master przetwarzajƒÖcy informacje z czujnik√≥w
- Komponenty pamiƒôci, planowania i dzia≈Çania

**Przyk≈Çad:**
```
Agent ‚Üí "Widzƒô osobƒô, powinienem siƒô przywitaƒá" ‚Üí Decyzja: "Powiedz: Witaj!"
```

### Warstwa 2: Middleware (Most - Warstwa Translacji)
**Co robi:** T≈Çumaczy miƒôdzy abstrakcyjnymi decyzjami a konkretnymi komendami robota.

**G≈Ç√≥wne zadania:**
- Konwersja danych z czujnik√≥w na obserwacje zrozumia≈Çe dla Concordia
- Translacja decyzji Concordia na konkretne komendy API robota
- Synchronizacja czasowa
- Obs≈Çuga b≈Çƒôd√≥w i sytuacji awaryjnych

**Przyk≈Çad:**
```
Concordia: "Powiedz: Witaj!" ‚Üí Middleware ‚Üí G1 API: play_audio("witaj.mp3") + gesture("wave")
```

### Warstwa 3: Unitree G1 API (Cia≈Ço - Warstwa Fizyczna)
**Co robi:** Bezpo≈õrednia kontrola nad robotem.

**Mo≈ºliwo≈õci:**
- Sterowanie stawami (joint control)
- Czytanie danych z czujnik√≥w (IMU, encodery, kamery)
- Kontrola g≈Çosowa (text-to-speech)
- Wykrywanie kolizji
- ZarzƒÖdzanie energiƒÖ

## Schemat Komunikacji

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CONCORDIA (Python)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ   Agent    ‚îÇ  ‚îÇ Game Master  ‚îÇ  ‚îÇ   Environment   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (Robot)   ‚îÇ  ‚îÇ (Interpreter)‚îÇ  ‚îÇ   (World)       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ        ‚îÇ                 ‚îÇ                    ‚îÇ              ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚îÇ Abstrakcyjne decyzje i obserwacje
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MIDDLEWARE (Python)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ           Sensor Data Processor                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Kamery ‚Üí Detekcja obiekt√≥w ‚Üí Obserwacje Concordia   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  IMU ‚Üí Orientacja ‚Üí Stan r√≥wnowagi                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Mikrofon ‚Üí Speech-to-Text ‚Üí Intencje u≈ºytkownika    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ           Action Translator                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  "Powiedz X" ‚Üí TTS + kontrola ust                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  "Id≈∫ naprz√≥d" ‚Üí Sekwencja krok√≥w                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  "Podnie≈õ obiekt" ‚Üí Trajektoria ramienia              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚îÇ Konkretne komendy i dane z czujnik√≥w
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  UNITREE G1 API (C++/Python)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Kontrola   ‚îÇ  ‚îÇ  Czujniki    ‚îÇ  ‚îÇ    Audio        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Staw√≥w     ‚îÇ  ‚îÇ  (Kamery,    ‚îÇ  ‚îÇ  (Mikrofon,     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ   IMU, etc)  ‚îÇ  ‚îÇ   G≈Ço≈õniki)     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                    ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                 ‚îÇ                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              UNITREE G1 EDU-U6 (Hardware)                    ‚îÇ
‚îÇ         Silniki, Czujniki, Aktuatory, Elektronika            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Implementacja Krok po Kroku

### Krok 1: Przygotowanie ≈örodowiska

#### 1.1 Instalacja Concordia
```bash
# Klonowanie repozytorium
git clone https://github.com/google-deepmind/concordia.git
cd concordia

# Tworzenie wirtualnego ≈õrodowiska
python -m venv venv_concordia
source venv_concordia/bin/activate  # Linux/Mac
# lub
venv_concordia\Scripts\activate  # Windows

# Instalacja
pip install --editable .[dev]
```

#### 1.2 Instalacja SDK Unitree G1
```bash
# Pobranie SDK Unitree (przyk≈Çadowa ≈õcie≈ºka - sprawd≈∫ dokumentacjƒô Unitree)
git clone https://github.com/unitreerobotics/unitree_sdk2_python.git
cd unitree_sdk2_python
pip install -e .
```

#### 1.3 Instalacja Dodatkowych Zale≈ºno≈õci
```bash
# Przetwarzanie obraz√≥w i d≈∫wiƒôku
pip install opencv-python
pip install pyaudio
pip install sounddevice
pip install pyttsx3  # Text-to-Speech
pip install speech_recognition  # Speech-to-Text

# Obs≈Çuga komunikacji
pip install websockets
pip install asyncio
```

### Krok 2: Implementacja Middleware

#### 2.1 Struktura Projektu
```
robot_integration/
‚îú‚îÄ‚îÄ concordia_agent/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ robot_agent.py      # Definicja agenta robota
‚îÇ   ‚îú‚îÄ‚îÄ robot_components.py  # Niestandardowe komponenty
‚îÇ   ‚îî‚îÄ‚îÄ robot_environment.py # ≈örodowisko dla robota
‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ sensor_processor.py  # Przetwarzanie danych z czujnik√≥w
‚îÇ   ‚îú‚îÄ‚îÄ action_translator.py # Translacja akcji na komendy
‚îÇ   ‚îú‚îÄ‚îÄ g1_interface.py      # Interfejs z API Unitree
‚îÇ   ‚îî‚îÄ‚îÄ safety_monitor.py    # Monitor bezpiecze≈Ñstwa
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ robot_config.yaml    # Konfiguracja robota
‚îÇ   ‚îî‚îÄ‚îÄ concordia_config.yaml # Konfiguracja Concordia
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_safety.py
‚îî‚îÄ‚îÄ main.py                  # G≈Ç√≥wny program
```

#### 2.2 Przyk≈Çadowy Kod: Interfejs z G1

```python
# middleware/g1_interface.py

"""
Interfejs do komunikacji z robotem Unitree G1.
Ten modu≈Ç zapewnia abstrakcjƒô nad niskopoziomowym API robota.
"""

import unitree_sdk2py  # SDK Unitree
import numpy as np
from typing import Dict, List, Optional
import logging

# Konfiguracja loggera dla ≈õledzenia dzia≈Çania
logger = logging.getLogger(__name__)


class UnitreeG1Interface:
    """
    Klasa obs≈ÇugujƒÖca komunikacjƒô z robotem Unitree G1 EDU-U6.
    
    G≈Ç√≥wne zadania:
    1. Inicjalizacja po≈ÇƒÖczenia z robotem
    2. Wysy≈Çanie komend ruchu i akcji
    3. Odbieranie danych z czujnik√≥w
    4. Monitorowanie stanu robota
    """
    
    def __init__(self, robot_ip: str = "192.168.123.10"):
        """
        Inicjalizacja interfejsu robota.
        
        Args:
            robot_ip: Adres IP robota w sieci lokalnej
        
        Dlaczego potrzebujemy IP?
        Robot komunikuje siƒô przez sieƒá, potrzebujemy znaƒá jego adres.
        """
        logger.info(f"Inicjalizacja po≈ÇƒÖczenia z robotem G1 pod adresem {robot_ip}")
        
        # Utworzenie klienta SDK
        # To jest nasz "telefon" do robota
        self.client = unitree_sdk2py.RobotClient(robot_ip)
        
        # Po≈ÇƒÖczenie z robotem
        if not self.client.connect():
            raise ConnectionError(f"Nie mo≈ºna po≈ÇƒÖczyƒá siƒô z robotem pod {robot_ip}")
        
        logger.info("Po≈ÇƒÖczenie z robotem nawiƒÖzane pomy≈õlnie")
        
        # Stan robota - przechowujemy ostatnie odczyty
        self.current_state = {
            'joint_positions': None,  # Pozycje staw√≥w
            'joint_velocities': None, # Prƒôdko≈õci staw√≥w
            'imu_data': None,         # Dane z ≈ºyroskopu/akcelerometru
            'battery_level': None,    # Poziom baterii
            'camera_frame': None      # Obraz z kamery
        }
        
    def get_sensor_data(self) -> Dict:
        """
        Pobiera aktualne dane ze wszystkich czujnik√≥w robota.
        
        Returns:
            S≈Çownik z danymi z czujnik√≥w
            
        Dlaczego to jest wa≈ºne?
        Agent Concordia musi "widzieƒá" ≈õwiat - dane z czujnik√≥w sƒÖ jego "oczami".
        """
        logger.debug("Pobieranie danych z czujnik√≥w")
        
        # Odczyt pozycji staw√≥w
        # Ka≈ºdy staw ma swojƒÖ pozycjƒô - to m√≥wi nam jak robot jest "z≈Ço≈ºony"
        joint_data = self.client.get_joint_states()
        self.current_state['joint_positions'] = joint_data.positions
        self.current_state['joint_velocities'] = joint_data.velocities
        
        # Odczyt IMU (Inertial Measurement Unit)
        # To m√≥wi nam czy robot stoi prosto, czy siƒô przewraca
        imu_data = self.client.get_imu_data()
        self.current_state['imu_data'] = {
            'orientation': imu_data.quaternion,  # Orientacja w przestrzeni
            'angular_velocity': imu_data.gyro,   # Jak szybko siƒô obraca
            'linear_acceleration': imu_data.accel # Jak szybko przy≈õpiesza
        }
        
        # Odczyt poziomu baterii
        # Wa≈ºne - robot musi wiedzieƒá kiedy musi siƒô na≈Çadowaƒá!
        battery = self.client.get_battery_state()
        self.current_state['battery_level'] = battery.percentage
        
        # Odczyt obrazu z kamery
        # To pozwoli robotowi "widzieƒá" otoczenie
        camera_frame = self.client.get_camera_frame()
        self.current_state['camera_frame'] = camera_frame
        
        return self.current_state.copy()
    
    def execute_action(self, action: Dict) -> bool:
        """
        Wykonuje akcjƒô na robocie na podstawie decyzji agenta.
        
        Args:
            action: S≈Çownik opisujƒÖcy akcjƒô do wykonania
                    Przyk≈Çad: {'type': 'speak', 'text': 'Witaj!'}
                    Przyk≈Çad: {'type': 'move', 'direction': 'forward', 'distance': 1.0}
        
        Returns:
            True je≈õli akcja zosta≈Ça wykonana pomy≈õlnie
            
        Dlaczego to jest wa≈ºne?
        To jest "most" miƒôdzy abstrakcyjnymi decyzjami agenta a fizycznymi ruchami robota.
        """
        action_type = action.get('type')
        logger.info(f"Wykonywanie akcji: {action_type}")
        
        try:
            if action_type == 'speak':
                # Akcja: Robot m√≥wi
                return self._speak(action['text'])
            
            elif action_type == 'move':
                # Akcja: Robot siƒô porusza
                return self._move(action['direction'], action.get('distance', 1.0))
            
            elif action_type == 'gesture':
                # Akcja: Robot wykonuje gest (np. machanie rƒôkƒÖ)
                return self._perform_gesture(action['gesture_name'])
            
            elif action_type == 'manipulate':
                # Akcja: Robot manipuluje obiektem (chwyta, podnosi)
                return self._manipulate_object(action)
            
            else:
                logger.warning(f"Nieznany typ akcji: {action_type}")
                return False
                
        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas wykonywania akcji: {e}")
            return False
    
    def _speak(self, text: str) -> bool:
        """
        Generuje mowƒô z tekstu i odtwarza przez g≈Ço≈õniki robota.
        
        Args:
            text: Tekst do wypowiedzenia
        
        Proces:
        1. Tekst ‚Üí Text-to-Speech ‚Üí Plik audio
        2. Plik audio ‚Üí Odtwarzacz ‚Üí G≈Ço≈õniki robota
        """
        logger.info(f"Robot m√≥wi: {text}")
        
        # Tutaj mo≈ºna u≈ºyƒá TTS (Text-to-Speech)
        # Przyk≈Çad z pyttsx3:
        # engine = pyttsx3.init()
        # engine.say(text)
        # engine.runAndWait()
        
        # Lub wys≈Çanie do API robota je≈õli ma wbudowany TTS
        return self.client.play_audio_text(text)
    
    def _move(self, direction: str, distance: float) -> bool:
        """
        Porusza robotem w okre≈õlonym kierunku.
        
        Args:
            direction: 'forward', 'backward', 'left', 'right'
            distance: Dystans w metrach
        
        Jak to dzia≈Ça:
        1. Planowanie trajektorii krok√≥w
        2. Obliczenie pozycji staw√≥w dla ka≈ºdego kroku
        3. Sekwencyjne wykonanie krok√≥w
        4. Monitorowanie r√≥wnowagi (IMU)
        """
        logger.info(f"Robot porusza siƒô {direction} o {distance}m")
        
        # Konwersja kierunku na wektor przemieszczenia
        displacement = self._direction_to_vector(direction, distance)
        
        # Wys≈Çanie komendy ruchu do robota
        return self.client.move_to_relative_position(displacement)
    
    def _perform_gesture(self, gesture_name: str) -> bool:
        """
        Wykonuje predefiniowany gest.
        
        Gestures mogƒÖ obejmowaƒá:
        - wave: Machanie rƒôkƒÖ na powitanie
        - thumbs_up: Kciuk w g√≥rƒô
        - point: Wskazywanie palcem
        - bow: Uk≈Çon
        """
        logger.info(f"Robot wykonuje gest: {gesture_name}")
        
        # Pobranie trajektorii gestu z biblioteki
        gesture_trajectory = self._get_gesture_trajectory(gesture_name)
        
        # Wykonanie trajektorii
        return self.client.execute_joint_trajectory(gesture_trajectory)
    
    def _manipulate_object(self, action: Dict) -> bool:
        """
        Manipulacja obiektami - chwytanie, przenoszenie, odk≈Çadanie.
        
        Args:
            action: Szczeg√≥≈Çy manipulacji
                    {'action': 'grasp', 'object_position': [x, y, z]}
        """
        manipulation_type = action.get('action')
        logger.info(f"Robot manipuluje: {manipulation_type}")
        
        if manipulation_type == 'grasp':
            # Chwytanie obiektu
            target_pos = action['object_position']
            return self._grasp_object(target_pos)
        
        elif manipulation_type == 'release':
            # Puszczanie obiektu
            return self._release_object()
        
        return False
    
    def _grasp_object(self, position: List[float]) -> bool:
        """
        Sekwencja chwytania obiektu:
        1. Planowanie trajektorii ramienia do pozycji obiektu
        2. Otworzenie chwytaka
        3. Ruch do obiektu
        4. Zamkniƒôcie chwytaka
        5. Podniesienie obiektu
        """
        logger.info(f"Chwytanie obiektu na pozycji {position}")
        
        # 1. Oblicz inverse kinematics (IK) - jakie pozycje staw√≥w sƒÖ potrzebne
        #    aby ramiƒô dotar≈Ço do pozycji obiektu
        joint_positions = self._compute_ik(position)
        
        # 2. Otw√≥rz chwytaki
        self.client.set_gripper_position(1.0)  # 1.0 = otwarty
        
        # 3. Przesu≈Ñ ramiƒô do pozycji
        self.client.move_arm_to_position(joint_positions)
        
        # 4. Zamknij chwytaki
        self.client.set_gripper_position(0.0)  # 0.0 = zamkniƒôty
        
        # 5. Podnie≈õ
        lift_position = joint_positions.copy()
        lift_position[2] += 0.1  # Podnie≈õ o 10cm
        self.client.move_arm_to_position(lift_position)
        
        return True
    
    def _compute_ik(self, target_position: List[float]) -> np.ndarray:
        """
        Inverse Kinematics - oblicza pozycje staw√≥w potrzebne
        aby effektor ko≈Ñcowy (np. d≈Ço≈Ñ) by≈Ç w docelowej pozycji.
        
        To jest z≈Ço≈ºone obliczenie matematyczne wykorzystujƒÖce
        model kinematyczny robota.
        """
        # Tutaj powinna byƒá faktyczna implementacja IK
        # Dla cel√≥w demonstracyjnych, zwracamy przyk≈ÇadowƒÖ warto≈õƒá
        return np.zeros(7)  # 7 staw√≥w ramienia
    
    def emergency_stop(self):
        """
        ZATRZYMANIE AWARYJNE
        
        To jest funkcja bezpiecze≈Ñstwa - zatrzymuje wszystkie ruchy robota
        natychmiast. U≈ºywana gdy:
        - Wykryto niebezpiecze≈Ñstwo
        - Cz≈Çowiek wchodzi w strefƒô robota
        - B≈ÇƒÖd w oprogramowaniu
        """
        logger.critical("EMERGENCY STOP ACTIVATED")
        self.client.stop_all_motion()
        self.client.disable_motors()
    
    def _direction_to_vector(self, direction: str, distance: float) -> np.ndarray:
        """Konwertuje kierunek tekstowy na wektor przemieszczenia."""
        vectors = {
            'forward': np.array([distance, 0, 0]),
            'backward': np.array([-distance, 0, 0]),
            'left': np.array([0, distance, 0]),
            'right': np.array([0, -distance, 0])
        }
        return vectors.get(direction, np.array([0, 0, 0]))
    
    def _get_gesture_trajectory(self, gesture_name: str) -> List:
        """Zwraca predefiniowanƒÖ trajektoriƒô dla gestu."""
        # Tutaj by≈Çyby zapisane trajektorie dla r√≥≈ºnych gest√≥w
        # Na razie zwracamy pustƒÖ listƒô
        return []
    
    def disconnect(self):
        """Bezpieczne roz≈ÇƒÖczenie z robotem."""
        logger.info("Roz≈ÇƒÖczanie z robotem")
        self.client.disconnect()
```

#### 2.3 Przyk≈Çadowy Kod: Przetwarzanie Czujnik√≥w

```python
# middleware/sensor_processor.py

"""
Przetwarza surowe dane z czujnik√≥w robota na obserwacje
zrozumia≈Çe dla agenta Concordia.
"""

import numpy as np
from typing import Dict, List
import cv2
import logging

logger = logging.getLogger(__name__)


class SensorProcessor:
    """
    Przetwarza dane z czujnik√≥w na wysoko-poziomowe obserwacje.
    
    Dlaczego to jest potrzebne?
    Agent Concordia operuje na poziomie abstrakcyjnym ("widzƒô osobƒô")
    a nie na poziomie surowych danych ("piksel[100,100] = [255,0,0]").
    """
    
    def __init__(self):
        """Inicjalizacja procesor√≥w dla r√≥≈ºnych typ√≥w czujnik√≥w."""
        logger.info("Inicjalizacja SensorProcessor")
        
        # Inicjalizacja detektora obiekt√≥w dla kamery
        # U≈ºywamy np. YOLO lub inny model wykrywania obiekt√≥w
        self.object_detector = self._init_object_detector()
        
        # Inicjalizacja rozpoznawania twarzy
        self.face_detector = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
    def process_camera(self, camera_frame: np.ndarray) -> Dict:
        """
        Przetwarza obraz z kamery na strukturyzowane obserwacje.
        
        Args:
            camera_frame: Surowy obraz z kamery (numpy array)
        
        Returns:
            S≈Çownik z wykrytymi obiektami i informacjami
            
        Proces:
        1. Obraz surowy ‚Üí Detekcja obiekt√≥w ‚Üí Lista obiekt√≥w
        2. Obraz surowy ‚Üí Detekcja twarzy ‚Üí Lista os√≥b
        3. Agregacja ‚Üí Opis sceny dla agenta
        """
        logger.debug("Przetwarzanie obrazu z kamery")
        
        observations = {
            'people_detected': [],
            'objects_detected': [],
            'scene_description': ""
        }
        
        # Wykrywanie twarzy (ludzi)
        gray = cv2.cvtColor(camera_frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_detector.detectMultiScale(
            gray, 
            scaleFactor=1.1, 
            minNeighbors=5
        )
        
        # Dla ka≈ºdej wykrytej twarzy
        for (x, y, w, h) in faces:
            person_info = {
                'position': (x + w//2, y + h//2),  # ≈örodek twarzy
                'distance': self._estimate_distance(w),  # Szacowana odleg≈Ço≈õƒá
                'is_looking_at_robot': self._is_facing_camera(x, y, w, h, camera_frame)
            }
            observations['people_detected'].append(person_info)
        
        # Wykrywanie innych obiekt√≥w
        # (kod dla detektora obiekt√≥w - np. YOLO)
        detected_objects = self._detect_objects(camera_frame)
        observations['objects_detected'] = detected_objects
        
        # Generowanie opisu sceny w jƒôzyku naturalnym
        observations['scene_description'] = self._generate_scene_description(
            observations
        )
        
        return observations
    
    def process_imu(self, imu_data: Dict) -> Dict:
        """
        Przetwarza dane z IMU (≈ºyroskop + akcelerometr).
        
        Args:
            imu_data: Surowe dane z IMU
        
        Returns:
            Wysoko-poziomowe informacje o stanie r√≥wnowagi
            
        Dlaczego to jest wa≈ºne?
        Robot musi wiedzieƒá czy stoi stabilnie, czy siƒô przewraca,
        czy mo≈ºe bezpiecznie wykonaƒá ruch.
        """
        logger.debug("Przetwarzanie danych IMU")
        
        # Oblicz kƒÖt pochylenia
        orientation = imu_data['orientation']
        tilt_angle = self._quaternion_to_euler(orientation)
        
        # Sprawd≈∫ czy robot jest w bezpiecznej pozycji
        is_stable = abs(tilt_angle[0]) < 10 and abs(tilt_angle[1]) < 10  # stopnie
        
        # Wykryj czy robot pada
        acceleration = imu_data['linear_acceleration']
        is_falling = self._detect_falling(acceleration, tilt_angle)
        
        return {
            'is_stable': is_stable,
            'is_falling': is_falling,
            'tilt_angle': tilt_angle,
            'balance_status': 'stable' if is_stable else 'unstable'
        }
    
    def process_all_sensors(self, sensor_data: Dict) -> str:
        """
        Agreguje wszystkie dane z czujnik√≥w w jeden opis tekstowy
        dla agenta Concordia.
        
        Args:
            sensor_data: Wszystkie surowe dane z czujnik√≥w
        
        Returns:
            Tekstowy opis obserwacji dla agenta
            
        Przyk≈Çadowy output:
        "Widzƒô 2 osoby w odleg≈Ço≈õci 3 metr√≥w. Jedna osoba patrzy na mnie.
         W pokoju znajduje siƒô st√≥≈Ç i krzes≈Ço. Robot stoi stabilnie."
        """
        # Przetwarzanie kamery
        camera_obs = self.process_camera(sensor_data['camera_frame'])
        
        # Przetwarzanie IMU
        imu_obs = self.process_imu(sensor_data['imu_data'])
        
        # Komponowanie opisu tekstowego
        description_parts = []
        
        # Ludzie
        num_people = len(camera_obs['people_detected'])
        if num_people > 0:
            description_parts.append(f"Widzƒô {num_people} os√≥b.")
            looking_count = sum(
                1 for p in camera_obs['people_detected'] 
                if p['is_looking_at_robot']
            )
            if looking_count > 0:
                description_parts.append(f"{looking_count} os√≥b patrzy na mnie.")
        
        # Obiekty
        if camera_obs['objects_detected']:
            objects_str = ", ".join([obj['name'] for obj in camera_obs['objects_detected']])
            description_parts.append(f"W otoczeniu znajduje siƒô: {objects_str}.")
        
        # Stan r√≥wnowagi
        description_parts.append(f"Robot {imu_obs['balance_status']}.")
        
        # Poziom baterii
        battery = sensor_data.get('battery_level', 100)
        if battery < 20:
            description_parts.append(f"UWAGA: Niski poziom baterii ({battery}%)!")
        
        return " ".join(description_parts)
    
    def _estimate_distance(self, face_width: int) -> float:
        """
        Szacuje odleg≈Ço≈õƒá do osoby na podstawie szeroko≈õci twarzy na obrazie.
        
        Zasada: Im wiƒôksza twarz na obrazie, tym bli≈ºej osoba.
        """
        # Za≈Ço≈ºenie: przeciƒôtna szeroko≈õƒá twarzy to ~15cm
        # Prosta formu≈Ça: distance = (known_width * focal_length) / pixel_width
        FACE_WIDTH_CM = 15
        FOCAL_LENGTH = 500  # Zale≈ºy od kamery, wymaga kalibracji
        
        distance_cm = (FACE_WIDTH_CM * FOCAL_LENGTH) / face_width
        return distance_cm / 100  # konwersja na metry
    
    def _is_facing_camera(self, x: int, y: int, w: int, h: int, 
                          frame: np.ndarray) -> bool:
        """
        Sprawdza czy osoba patrzy w stronƒô kamery.
        
        Uproszczona wersja - sprawdza czy twarz jest centralna i wyra≈∫na.
        Zaawansowana wersja mog≈Çaby u≈ºywaƒá detekcji kierunku spojrzenia.
        """
        frame_center_x = frame.shape[1] // 2
        face_center_x = x + w // 2
        
        # Je≈õli ≈õrodek twarzy jest blisko ≈õrodka kadru, osoba prawdopodobnie patrzy na robot
        distance_from_center = abs(face_center_x - frame_center_x)
        return distance_from_center < frame.shape[1] * 0.2  # w promieniu 20% szeroko≈õci
    
    def _detect_objects(self, frame: np.ndarray) -> List[Dict]:
        """
        Wykrywa obiekty na obrazie u≈ºywajƒÖc modelu detekcji obiekt√≥w.
        
        Tutaj mo≈ºna u≈ºyƒá YOLO, Faster R-CNN lub inny model.
        """
        # Przyk≈Çadowa implementacja (placeholder)
        # W rzeczywisto≈õci u≈ºy≈Çby≈õ modelu ML do detekcji
        detected = []
        
        # objects = self.object_detector.detect(frame)
        # for obj in objects:
        #     detected.append({
        #         'name': obj.class_name,
        #         'confidence': obj.confidence,
        #         'position': obj.bounding_box
        #     })
        
        return detected
    
    def _generate_scene_description(self, observations: Dict) -> str:
        """Generuje naturalny opis sceny na podstawie obserwacji."""
        # Mo≈ºna u≈ºyƒá prostych regu≈Ç lub modelu generujƒÖcego opisy
        return "Scena z lud≈∫mi i obiektami"
    
    def _quaternion_to_euler(self, quaternion: List[float]) -> List[float]:
        """Konwertuje kwaternion na kƒÖty Eulera (roll, pitch, yaw)."""
        # Implementacja konwersji kwaternion ‚Üí Euler
        # Uproszczona wersja
        return [0.0, 0.0, 0.0]  # [roll, pitch, yaw] w stopniach
    
    def _detect_falling(self, acceleration: List[float], tilt: List[float]) -> bool:
        """
        Wykrywa czy robot pada.
        
        Sygna≈Çy upadku:
        - Nag≈Çy wzrost przyspieszenia
        - Du≈ºy kƒÖt pochylenia
        - Szybka zmiana orientacji
        """
        # Je≈õli przyspieszenie przekracza pr√≥g (robot spada)
        accel_magnitude = np.linalg.norm(acceleration)
        if accel_magnitude > 15.0:  # m/s¬≤ (przyspieszenie wiƒôksze ni≈º grawitacja)
            return True
        
        # Je≈õli kƒÖt pochylenia jest zbyt du≈ºy
        if abs(tilt[0]) > 30 or abs(tilt[1]) > 30:  # stopnie
            return True
        
        return False
    
    def _init_object_detector(self):
        """Inicjalizacja modelu detekcji obiekt√≥w."""
        # Tutaj za≈Çadowa≈Çby≈õ model YOLO lub inny
        return None
```

#### 2.4 Przyk≈Çadowy Kod: Translator Akcji

```python
# middleware/action_translator.py

"""
T≈Çumaczy abstrakcyjne akcje z Concordia na konkretne komendy dla robota.
"""

import logging
from typing import Dict, List

logger = logging.getLogger(__name__)


class ActionTranslator:
    """
    Translator miƒôdzy jƒôzykiem naturalnym akcji a komendami robota.
    
    Proces:
    Concordia: "Robot powinien siƒô przywitaƒá" 
    ‚Üí ActionTranslator: {'type': 'gesture', 'name': 'wave'} + {'type': 'speak', 'text': 'Witam'}
    ‚Üí G1Interface: Wykonanie gestu machania + odtworzenie audio
    """
    
    def __init__(self):
        """Inicjalizacja translatora z mapowaniem akcji."""
        logger.info("Inicjalizacja ActionTranslator")
        
        # S≈Çownik mapujƒÖcy intencje na akcje
        self.action_mappings = {
            'greet': self._create_greeting_action,
            'move_forward': self._create_move_forward_action,
            'pick_object': self._create_pick_object_action,
            'answer_question': self._create_answer_action,
            'wave': self._create_wave_action,
        }
    
    def translate_agent_action(self, agent_output: str) -> List[Dict]:
        """
        T≈Çumaczy output agenta Concordia na listƒô akcji do wykonania.
        
        Args:
            agent_output: Tekst wygenerowany przez agenta
                         Przyk≈Çad: "Powinienem przywitaƒá osobƒô i zapytaƒá czy mogƒô pom√≥c"
        
        Returns:
            Lista s≈Çownik√≥w z akcjami do wykonania
            Przyk≈Çad: [
                {'type': 'speak', 'text': 'Witam!'},
                {'type': 'gesture', 'name': 'wave'},
                {'type': 'speak', 'text': 'Czy mogƒô w czym≈õ pom√≥c?'}
            ]
        """
        logger.info(f"T≈Çumaczenie akcji agenta: {agent_output}")
        
        actions = []
        
        # Analiza output agenta i ekstrakcja intencji
        # To mo≈ºe byƒá proste parsowanie tekstu lub u≈ºycie NLP
        intent = self._extract_intent(agent_output)
        
        # Pobranie odpowiedniego translatora dla intencji
        action_creator = self.action_mappings.get(intent)
        
        if action_creator:
            actions = action_creator(agent_output)
        else:
            # Je≈õli nie znamy intencji, robot mo≈ºe tylko powt√≥rzyƒá co agent powiedzia≈Ç
            logger.warning(f"Nieznana intencja: {intent}")
            actions = [{'type': 'speak', 'text': agent_output}]
        
        return actions
    
    def _extract_intent(self, text: str) -> str:
        """
        Ekstrakcja intencji z tekstu agenta.
        
        Uproszczona wersja - sprawdza s≈Çowa kluczowe.
        Zaawansowana wersja mog≈Çaby u≈ºyƒá modelu NLU (Natural Language Understanding).
        """
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['witaj', 'cze≈õƒá', 'dzie≈Ñ dobry', 'przywitaƒá']):
            return 'greet'
        elif any(word in text_lower for word in ['id≈∫', 'porusz', 'naprz√≥d', 'do przodu']):
            return 'move_forward'
        elif any(word in text_lower for word in ['podnie≈õ', 'we≈∫', 'chwyƒá', 'z≈Çap']):
            return 'pick_object'
        elif any(word in text_lower for word in ['odpowiedz', 'powiedz', 'wyja≈õnij']):
            return 'answer_question'
        elif any(word in text_lower for word in ['pomachaj', 'wave']):
            return 'wave'
        
        return 'unknown'
    
    def _create_greeting_action(self, text: str) -> List[Dict]:
        """Tworzy sekwencjƒô akcji powitania."""
        return [
            {'type': 'gesture', 'name': 'wave'},
            {'type': 'speak', 'text': 'Witam! Mi≈Ço Ciƒô widzieƒá.'},
            {'type': 'gesture', 'name': 'slight_bow'}
        ]
    
    def _create_move_forward_action(self, text: str) -> List[Dict]:
        """Tworzy akcjƒô ruchu do przodu."""
        # Mo≈ºna wyekstrahowaƒá dystans z tekstu
        distance = self._extract_distance(text)
        return [
            {'type': 'speak', 'text': f'Poruszam siƒô do przodu o {distance} metr.'},
            {'type': 'move', 'direction': 'forward', 'distance': distance}
        ]
    
    def _create_pick_object_action(self, text: str) -> List[Dict]:
        """Tworzy sekwencjƒô akcji podnoszenia obiektu."""
        # W pe≈Çnej implementacji wykry≈Çby≈õ pozycjƒô obiektu z kamery
        return [
            {'type': 'speak', 'text': 'Podnoszƒô obiekt.'},
            {'type': 'manipulate', 'action': 'grasp', 'object_position': [0.5, 0, 0.3]},
            {'type': 'speak', 'text': 'Mam obiekt.'}
        ]
    
    def _create_answer_action(self, text: str) -> List[Dict]:
        """Tworzy akcjƒô odpowiedzi na pytanie."""
        # Wyekstrahuj odpowied≈∫ z tekstu agenta
        answer = self._extract_answer(text)
        return [
            {'type': 'speak', 'text': answer}
        ]
    
    def _create_wave_action(self, text: str) -> List[Dict]:
        """Tworzy akcjƒô machania rƒôkƒÖ."""
        return [
            {'type': 'gesture', 'name': 'wave'}
        ]
    
    def _extract_distance(self, text: str) -> float:
        """Ekstrahuje dystans z tekstu."""
        # Uproszczona ekstrakcja - szuka liczb
        import re
        numbers = re.findall(r'\d+\.?\d*', text)
        return float(numbers[0]) if numbers else 1.0  # domy≈õlnie 1 metr
    
    def _extract_answer(self, text: str) -> str:
        """Ekstrahuje odpowied≈∫ z tekstu agenta."""
        # W tym przypadku, ca≈Çy tekst jest odpowiedziƒÖ
        return text
```

### Krok 3: Integracja z Concordia

#### 3.1 Tworzenie Agenta Robota

```python
# concordia_agent/robot_agent.py

"""
Definicja agenta Concordia reprezentujƒÖcego robota Unitree G1.
"""

from concordia import components
from concordia.prefabs import entity as entity_prefabs
from concordia.typing import entity as entity_lib
import logging

logger = logging.getLogger(__name__)


def create_robot_agent(
    name: str = "RobotG1",
    model: any = None,  # Model jƒôzykowy (LLM)
    embedder: any = None,  # Embedder dla pamiƒôci
) -> entity_lib.Entity:
    """
    Tworzy agenta reprezentujƒÖcego robota Unitree G1.
    
    Args:
        name: Nazwa robota
        model: Model jƒôzykowy do generowania decyzji
        embedder: Model do tworzenia embedding√≥w dla pamiƒôci
    
    Returns:
        Skonfigurowany agent robota
        
    Architektura agenta:
    - Pamiƒôƒá: Co robot pamiƒôta (poprzednie interakcje, zadania)
    - Obserwacja: Co robot widzi i czuje (dane z czujnik√≥w)
    - Planowanie: Jak robot decyduje co zrobiƒá
    - Dzia≈Çanie: Co robot faktycznie robi
    """
    logger.info(f"Tworzenie agenta robota: {name}")
    
    # Opis robota - to kszta≈Çtuje jego "osobowo≈õƒá"
    robot_description = """
    Jestem robotem humanoidalnym Unitree G1 pracujƒÖcym w laboratorium 
    Politechniki Rzeszowskiej. Moim celem jest asystowanie studentom 
    i pracownikom uczelni.
    
    Moje cechy:
    - Przyjazny i pomocny
    - Bezpieczny (zawsze dbam o bezpiecze≈Ñstwo ludzi)
    - Komunikatywny (lubiƒô rozmawiaƒá z lud≈∫mi)
    - Staranny (wykonujƒô zadania precyzyjnie)
    
    Moje mo≈ºliwo≈õci:
    - Poruszanie siƒô po pomieszczeniach
    - Manipulowanie obiektami
    - Prowadzenie konwersacji
    - Rozpoznawanie os√≥b i obiekt√≥w
    """
    
    # Tworzenie komponent√≥w agenta
    
    # 1. PAMIƒòƒÜ - Co robot pamiƒôta
    memory = components.memory.AssociativeMemory(
        model=model,
        embedder=embedder,
        clock=None,  # Bƒôdzie dodany p√≥≈∫niej
        add_time=True
    )
    
    # 2. OBSERWACJA - Co robot postrzega
    # Ten komponent bƒôdzie otrzymywaƒá przetworzone dane z czujnik√≥w
    observation = components.observation.Observation(
        agent_name=name,
        clock=None,  # Bƒôdzie dodany p√≥≈∫niej
        memory=memory,
    )
    
    # 3. PLANOWANIE - Jak robot my≈õli
    # Inspirowane March & Olsen (2011) - trzy pytania:
    # - Jaka jest sytuacja?
    # - Kim jestem?
    # - Co powinienem zrobiƒá?
    
    situation_perception = components.agent.SituationPerception(
        model=model,
        memory=memory,
        agent_name=name,
        components=[]
    )
    
    self_perception = components.agent.SelfPerception(
        model=model,
        agent_name=name,
        description=robot_description
    )
    
    plan = components.agent.Plan(
        model=model,
        observation=observation,
        memory=memory,
    )
    
    # 4. DZIA≈ÅANIE - Co robot robi
    # Ten komponent generuje akcje kt√≥re p√≥≈∫niej sƒÖ t≈Çumaczone na komendy robota
    action = components.agent.ActComponent(
        model=model,
        memory=memory,
    )
    
    # 5. KOMPONENTY DODATKOWE specyficzne dla robota
    
    # Monitor bezpiecze≈Ñstwa - ciƒÖgle sprawdza czy robot jest bezpieczny
    safety_monitor = create_safety_component(model, memory)
    
    # Monitor baterii - ≈õledzi poziom energii
    battery_monitor = create_battery_component(model, memory)
    
    # Z≈Ço≈ºenie wszystkich komponent√≥w w agenta
    agent = entity_prefabs.build_agent(
        name=name,
        act_component=action,
        observation=observation,
        memory=memory,
        components=[
            situation_perception,
            self_perception,
            plan,
            safety_monitor,
            battery_monitor,
        ]
    )
    
    logger.info(f"Agent robota {name} utworzony pomy≈õlnie")
    
    return agent


def create_safety_component(model, memory):
    """
    Komponent monitorujƒÖcy bezpiecze≈Ñstwo robota.
    
    Zadania:
    - Sprawdzanie czy robot stoi stabilnie
    - Wykrywanie ludzi w strefie roboczej
    - Blokowanie niebezpiecznych akcji
    """
    # Implementacja komponentu bezpiecze≈Ñstwa
    # To by≈Çby niestandardowy komponent
    pass


def create_battery_component(model, memory):
    """
    Komponent monitorujƒÖcy poziom baterii.
    
    Zadania:
    - ≈öledzenie poziomu na≈Çadowania
    - Ostrzeganie gdy bateria jest niska
    - Inicjowanie powrotu do stacji ≈Çadowania
    """
    # Implementacja komponentu baterii
    pass
```

### Krok 4: G≈Ç√≥wny Program Integracyjny

```python
# main.py

"""
G≈Ç√≥wny program ≈ÇƒÖczƒÖcy Concordia z robotem Unitree G1.
"""

import asyncio
import logging
from typing import Dict

# Importy z Concordia
from concordia.contrib import language_models
from concordia.prefabs.simulation import generic as simulation

# Importy z middleware
from middleware.g1_interface import UnitreeG1Interface
from middleware.sensor_processor import SensorProcessor
from middleware.action_translator import ActionTranslator

# Importy agenta robota
from concordia_agent.robot_agent import create_robot_agent

# Konfiguracja loggera
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RobotConcordiaIntegration:
    """
    G≈Ç√≥wna klasa integrujƒÖca Concordia z robotem Unitree G1.
    
    Przep≈Çyw dzia≈Çania:
    1. Odczyt czujnik√≥w robota
    2. Przetworzenie na obserwacje dla agenta
    3. Agent podejmuje decyzjƒô (u≈ºywajƒÖc Concordia)
    4. Translacja decyzji na komendy robota
    5. Wykonanie komend na robocie
    6. Powr√≥t do kroku 1
    """
    
    def __init__(self, config: Dict):
        """
        Inicjalizacja systemu integracyjnego.
        
        Args:
            config: S≈Çownik konfiguracyjny zawierajƒÖcy:
                - robot_ip: Adres IP robota
                - llm_api_key: Klucz API do modelu jƒôzykowego
                - llm_model: Nazwa modelu jƒôzykowego
        """
        logger.info("Inicjalizacja RobotConcordiaIntegration")
        
        self.config = config
        
        # 1. Inicjalizacja interfejsu z robotem
        self.robot = UnitreeG1Interface(config['robot_ip'])
        
        # 2. Inicjalizacja procesora czujnik√≥w
        self.sensor_processor = SensorProcessor()
        
        # 3. Inicjalizacja translatora akcji
        self.action_translator = ActionTranslator()
        
        # 4. Inicjalizacja modelu jƒôzykowego dla Concordia
        self.model, self.embedder = self._init_language_model()
        
        # 5. Utworzenie agenta robota
        self.agent = create_robot_agent(
            name="RobotG1_PRz",  # PRz = Politechnika Rzeszowska
            model=self.model,
            embedder=self.embedder
        )
        
        # Stan systemu
        self.is_running = False
        self.loop_delay = config.get('loop_delay', 0.5)  # sekund miƒôdzy cyklami
        
        logger.info("Inicjalizacja zako≈Ñczona pomy≈õlnie")
    
    def _init_language_model(self):
        """
        Inicjalizacja modelu jƒôzykowego i embedddera.
        
        Concordia wymaga:
        - Model jƒôzykowy (LLM) do generowania decyzji
        - Embedder do tworzenia reprezentacji wektorowych dla pamiƒôci
        """
        logger.info("Inicjalizacja modelu jƒôzykowego")
        
        # Konfiguracja modelu jƒôzykowego
        model = language_models.language_model_setup(
            api_key=self.config['llm_api_key'],
            model_name=self.config['llm_model'],
            api_type=self.config.get('api_type', 'openai')
        )
        
        # Konfiguracja embedddera (np. sentence-transformers)
        import sentence_transformers
        embedder = sentence_transformers.SentenceTransformer(
            'all-MiniLM-L6-v2'
        )
        
        return model, embedder
    
    async def run(self):
        """
        G≈Ç√≥wna pƒôtla integracyjna.
        
        To jest "serce" systemu - ciƒÖgle:
        1. Czyta czujniki
        2. Przetwarza na obserwacje
        3. Agent my≈õli
        4. Wykonuje akcje
        5. Powtarza
        """
        logger.info("Uruchamianie g≈Ç√≥wnej pƒôtli integracyjnej")
        self.is_running = True
        
        try:
            while self.is_running:
                # ===== KROK 1: PERCEPCJA =====
                # Odczyt danych z czujnik√≥w robota
                sensor_data = self.robot.get_sensor_data()
                
                # Przetworzenie na obserwacje zrozumia≈Çe dla agenta
                observation_text = self.sensor_processor.process_all_sensors(
                    sensor_data
                )
                
                logger.info(f"Obserwacja: {observation_text}")
                
                # ===== KROK 2: DELIBERACJA =====
                # Agent analizuje sytuacjƒô i podejmuje decyzjƒô
                agent_action = self.agent.act(observation_text)
                
                logger.info(f"Decyzja agenta: {agent_action}")
                
                # ===== KROK 3: TRANSLACJA =====
                # T≈Çumaczenie decyzji agenta na konkretne akcje robota
                robot_actions = self.action_translator.translate_agent_action(
                    agent_action
                )
                
                logger.info(f"Akcje robota: {robot_actions}")
                
                # ===== KROK 4: WYKONANIE =====
                # Wykonanie akcji na robocie
                for action in robot_actions:
                    success = self.robot.execute_action(action)
                    if not success:
                        logger.warning(f"Akcja nie powiod≈Ça siƒô: {action}")
                
                # Op√≥≈∫nienie przed nastƒôpnƒÖ iteracjƒÖ
                await asyncio.sleep(self.loop_delay)
                
        except KeyboardInterrupt:
            logger.info("Przerwanie przez u≈ºytkownika")
        except Exception as e:
            logger.error(f"B≈ÇƒÖd w g≈Ç√≥wnej pƒôtli: {e}", exc_info=True)
        finally:
            self.shutdown()
    
    def shutdown(self):
        """Bezpieczne wy≈ÇƒÖczenie systemu."""
        logger.info("Wy≈ÇƒÖczanie systemu")
        self.is_running = False
        
        # Zatrzymanie robota
        self.robot.emergency_stop()
        
        # Roz≈ÇƒÖczenie
        self.robot.disconnect()
        
        logger.info("System wy≈ÇƒÖczony")


# ===== PUNKT WEJ≈öCIA PROGRAMU =====

if __name__ == "__main__":
    # Konfiguracja
    config = {
        'robot_ip': '192.168.123.10',  # Adres IP robota Unitree G1
        'llm_api_key': 'YOUR_API_KEY_HERE',  # Tw√≥j klucz API
        'llm_model': 'gpt-4',  # Model jƒôzykowy
        'api_type': 'openai',
        'loop_delay': 0.5  # Op√≥≈∫nienie miƒôdzy cyklami (sekundy)
    }
    
    # Utworzenie systemu integracyjnego
    integration = RobotConcordiaIntegration(config)
    
    # Uruchomienie
    asyncio.run(integration.run())
```

## Testowanie Integracji

### Test 1: Symulacja Bez Fizycznego Robota

Przed testami na prawdziwym robocie, przetestuj system z "wirtualnym" robotem:

```python
# tests/test_integration.py

"""Testy integracji bez fizycznego robota."""

class MockRobotInterface:
    """Symulowany robot do test√≥w."""
    
    def get_sensor_data(self):
        """Zwraca przyk≈Çadowe dane z czujnik√≥w."""
        import numpy as np
        return {
            'camera_frame': np.random.rand(480, 640, 3),
            'imu_data': {
                'orientation': [1, 0, 0, 0],
                'angular_velocity': [0, 0, 0],
                'linear_acceleration': [0, 0, 9.81]
            },
            'battery_level': 85
        }
    
    def execute_action(self, action):
        """Symuluje wykonanie akcji."""
        print(f"[MOCK ROBOT] Wykonujƒô: {action}")
        return True


# Test
if __name__ == "__main__":
    mock_robot = MockRobotInterface()
    
    # Test odczytu czujnik√≥w
    data = mock_robot.get_sensor_data()
    print(f"Dane z czujnik√≥w: {data}")
    
    # Test wykonania akcji
    mock_robot.execute_action({'type': 'speak', 'text': 'Test'})
```

### Test 2: Test na Prawdziwym Robocie (Stopniowo)

1. **Faza 1**: Test tylko odczytu czujnik√≥w
2. **Faza 2**: Test prostych akcji (m√≥wienie)
3. **Faza 3**: Test ruch√≥w (poruszanie siƒô)
4. **Faza 4**: Test z≈Ço≈ºonych zachowa≈Ñ

## Bezpiecze≈Ñstwo

### Zasady Bezpiecze≈Ñstwa

1. **Zawsze miej przycisk STOP** - Fizyczny przycisk awaryjnego zatrzymania
2. **Testuj w bezpiecznej przestrzeni** - Obszar bez ludzi i cennych przedmiot√≥w
3. **Monitoruj ciƒÖgle** - Kto≈õ zawsze obserwuje robota podczas test√≥w
4. **Ograniczenia prƒôdko≈õci** - Na poczƒÖtku test√≥w ogranicz maksymalnƒÖ prƒôdko≈õƒá
5. **Tryb symulacji** - Zawsze najpierw testuj w symulacji

### Implementacja Monitor Bezpiecze≈Ñstwa

```python
# middleware/safety_monitor.py

class SafetyMonitor:
    """Monitor bezpiecze≈Ñstwa robota."""
    
    def check_safety(self, sensor_data: Dict, planned_action: Dict) -> bool:
        """
        Sprawdza czy planowana akcja jest bezpieczna.
        
        Returns:
            True je≈õli akcja jest bezpieczna, False je≈õli nale≈ºy jƒÖ zablokowaƒá
        """
        # Sprawd≈∫ poziom baterii
        if sensor_data['battery_level'] < 10:
            logger.critical("CRITICAL: Bardzo niski poziom baterii!")
            return False
        
        # Sprawd≈∫ stabilno≈õƒá
        imu_obs = process_imu(sensor_data['imu_data'])
        if not imu_obs['is_stable']:
            logger.warning("WARNING: Robot niestabilny, blokowanie ruchu")
            return False
        
        # Sprawd≈∫ czy sƒÖ ludzie w strefie roboczej
        camera_obs = process_camera(sensor_data['camera_frame'])
        if planned_action['type'] == 'move':
            for person in camera_obs['people_detected']:
                if person['distance'] < 1.0:  # 1 metr
                    logger.warning("WARNING: Cz≈Çowiek zbyt blisko, blokowanie ruchu")
                    return False
        
        return True
```

## Troubleshooting

### Problem 1: Robot siƒô nie ≈ÇƒÖczy
**RozwiƒÖzanie:**
- Sprawd≈∫ IP robota (ping)
- Sprawd≈∫ czy robot jest w≈ÇƒÖczony
- Sprawd≈∫ czy jeste≈õ w tej samej sieci

### Problem 2: Agent Concordia nie reaguje logicznie
**RozwiƒÖzanie:**
- Sprawd≈∫ prompt—ã - czy opis agenta jest jasny
- Sprawd≈∫ obserwacje - czy agent dostaje sensowne informacje
- Dodaj wiƒôcej przyk≈Çad√≥w do pamiƒôci agenta

### Problem 3: Robot wykonuje niebezpieczne ruchy
**RozwiƒÖzanie:**
- NATYCHMIAST u≈ºyj przycisku STOP
- Przejrzyj logi - co agent "my≈õla≈Ç"
- Wzmocnij safety monitor
- Ogranicz mo≈ºliwe akcje

## Dalsze Kroki

1. **Optymalizacja**: Zredukuj op√≥≈∫nienia w pƒôtli
2. **Uczenie siƒô**: Dodaj mechanizm uczenia siƒô z do≈õwiadczenia
3. **Multimodalno≈õƒá**: Dodaj wiƒôcej typ√≥w czujnik√≥w
4. **Spo≈Çeczno≈õƒá**: Po≈ÇƒÖcz z innymi robotami (multi-agent)

## Zako≈Ñczenie

Ta integracja to dopiero poczƒÖtek! Mo≈ºesz rozwijaƒá system w wielu kierunkach:
- Bardziej zaawansowane rozumienie jƒôzyka
- Lepsze planowanie d≈Çugoterminowe
- Uczenie przez demonstracjƒô
- Wsp√≥≈Çpraca miƒôdzy wieloma robotami

Powodzenia w projekcie! ü§ñüöÄ
